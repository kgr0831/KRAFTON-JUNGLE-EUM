"use client";

import { useState, useEffect, useRef, useMemo } from "react";
import { useParticipants, useLocalParticipant, useTracks } from "@livekit/components-react";
import { Track, RemoteParticipant, LocalParticipant } from "livekit-client";
import { TranscriptData, TargetLanguage } from "./useAudioWebSocket";
import { useAudioPlayback } from "./useAudioPlayback";
import { useAudioDucking } from "./useAudioDucking";

const WS_BASE_URL = process.env.NEXT_PUBLIC_VOICE_WS_URL || 'ws://localhost:8080/ws/audio';
const SAMPLE_RATE = Number(process.env.NEXT_PUBLIC_AUDIO_SAMPLE_RATE) || 16000;

// ì‹¤ì‹œê°„ ë²ˆì—­ ì„¤ì •: ë¬¸ì¥ ì™„ì„±ë„ vs ì†ë„ ë°¸ëŸ°ìŠ¤
const MIN_SAMPLES_TO_SEND = 0;  // ëª¨ë“  ì˜¤ë””ì˜¤ ì „ì†¡ (ìŠ¤í‚µ ì—†ìŒ)
const MAX_BUFFER_DURATION_MS = 5000;  // ìµœëŒ€ 5ì´ˆ ë²„í¼ (ì•ˆì „ì¥ì¹˜)
const MAX_BUFFER_SAMPLES = SAMPLE_RATE * (MAX_BUFFER_DURATION_MS / 1000);
const SILENCE_THRESHOLD = 0.004;  // ì¹¨ë¬µ ê°ì§€ RMS ì„ê³„ê°’ (ë¯¼ê°í•˜ê²Œ)
const SPEECH_THRESHOLD = 0.006;  // ë°œí™” ì‹œì‘ ê°ì§€ RMS ì„ê³„ê°’
const SILENCE_DURATION_MS = 350;  // 350ms ì¹¨ë¬µ = ë°œí™” ì¢…ë£Œ (ì¹¨ë¬µ ê°ì§€ë¡œ ë¹ ë¥´ê²Œ)
const FORCED_SEND_INTERVAL_MS = 2500;  // 2.5ì´ˆë§ˆë‹¤ ê°•ì œ ì „ì†¡ (ë¬¸ì¥ ì™„ì„± ëŒ€ê¸°)
const ANALYSIS_INTERVAL_MS = 30;  // 30msë§ˆë‹¤ ë²„í¼ ë¶„ì„

export interface RemoteTranscriptData extends TranscriptData {
    participantId: string;
    participantName?: string;
    profileImg?: string;
}

interface UseRemoteParticipantTranslationOptions {
    roomId: string;                // Room ID for grouping
    enabled: boolean;              // TTS ì¬ìƒ ì—¬ë¶€ (ë²ˆì—­ ëª¨ë“œ)
    sttEnabled?: boolean;          // STT í•­ìƒ í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸: true)
    sourceLanguage?: TargetLanguage;  // ë°œí™”ìê°€ ë§í•˜ëŠ” ì–¸ì–´ (ê¸°ë³¸: 'ko')
    targetLanguage?: TargetLanguage;  // ë“£ê³  ì‹¶ì€ ì–¸ì–´ (ë²ˆì—­ ëŒ€ìƒ)
    listenerId?: string;           // ë¦¬ìŠ¤ë„ˆ ID (ë¡œì»¬ ì‚¬ìš©ì)
    autoPlayTTS?: boolean;
    onTranscript?: (data: RemoteTranscriptData) => void;
    onError?: (error: Error) => void;
}

// WebSocket ì¬ì—°ê²° ì„¤ì •
const WS_RECONNECT_MAX_RETRIES = 5;
const WS_RECONNECT_BASE_DELAY_MS = 1000;
const WS_RECONNECT_MAX_DELAY_MS = 30000;

interface ParticipantStream {
    participantId: string;
    ws: WebSocket;
    audioContext: AudioContext;
    sourceNode: MediaStreamAudioSourceNode | null;
    workletNode: AudioWorkletNode | null;
    audioBuffer: Float32Array[];
    analysisInterval: NodeJS.Timeout | null;
    isHandshakeComplete: boolean;
    // ì¹¨ë¬µ ê°ì§€ ê´€ë ¨ ìƒíƒœ
    isSpeaking: boolean;
    silenceStartTime: number | null;
    lastSpeechTime: number;
    // ì˜¤ë””ì˜¤ ì†ì‹¤ ë°©ì§€ë¥¼ ìœ„í•œ ê°•ì œ ì „ì†¡ íƒ€ì´ë¨¸
    lastSendTime: number;
    // WebSocket ì¬ì—°ê²° ê´€ë ¨ ìƒíƒœ
    reconnectAttempts: number;
    reconnectTimeoutId: NodeJS.Timeout | null;
    isIntentionalClose: boolean;
    mediaStream: MediaStream | null;
    remoteSourceLang: TargetLanguage;
}

interface UseRemoteParticipantTranslationReturn {
    isActive: boolean;
    activeParticipantCount: number;
    transcripts: Map<string, RemoteTranscriptData>;
    error: Error | null;
}

// 12 byte metadata header (Little Endian)
function createMetadataHeader(sampleRate: number, channels: number, bitsPerSample: number): ArrayBuffer {
    const buffer = new ArrayBuffer(12);
    const view = new DataView(buffer);
    view.setUint32(0, sampleRate, true);
    view.setUint16(4, channels, true);
    view.setUint16(6, bitsPerSample, true);
    view.setUint32(8, 0, true);
    return buffer;
}

// Float32 -> Int16 PCM
function float32ToInt16(float32Array: Float32Array): Int16Array {
    const int16Array = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
}

// Parse participant metadata to extract profileImg
function getParticipantProfileImg(participant: RemoteParticipant | LocalParticipant): string | undefined {
    try {
        if (participant.metadata) {
            const metadata = JSON.parse(participant.metadata);
            return metadata.profileImg || metadata.profile_img || metadata.avatar;
        }
    } catch {
        // Metadata is not valid JSON
    }
    return undefined;
}

// Parse participant metadata to extract sourceLanguage (the language they speak)
// Returns null if metadata is not available (to distinguish from fallback)
function getParticipantSourceLanguage(participant: RemoteParticipant | LocalParticipant): TargetLanguage | null {
    try {
        if (participant.metadata) {
            const metadata = JSON.parse(participant.metadata);
            // Check various possible field names for source language
            const lang = metadata.sourceLanguage || metadata.source_language || metadata.speakingLanguage || metadata.language;
            if (lang && typeof lang === 'string') {
                return lang as TargetLanguage;
            }
        }
    } catch {
        // Metadata is not valid JSON
    }
    return null; // No metadata available - caller should decide how to handle
}

// RMS ê³„ì‚° (ìŒì„± í™œë™ ê°ì§€ìš©)
function calculateRMS(samples: Float32Array): number {
    if (samples.length === 0) return 0;
    let sum = 0;
    for (let i = 0; i < samples.length; i++) {
        sum += samples[i] * samples[i];
    }
    return Math.sqrt(sum / samples.length);
}

// Linear interpolation resampling
function resample(inputBuffer: Float32Array, inputSampleRate: number, outputSampleRate: number): Float32Array {
    if (inputSampleRate === outputSampleRate) {
        return inputBuffer;
    }

    const ratio = inputSampleRate / outputSampleRate;
    const outputLength = Math.floor(inputBuffer.length / ratio);
    const output = new Float32Array(outputLength);

    for (let i = 0; i < outputLength; i++) {
        const srcIndex = i * ratio;
        const srcIndexFloor = Math.floor(srcIndex);
        const srcIndexCeil = Math.min(srcIndexFloor + 1, inputBuffer.length - 1);
        const fraction = srcIndex - srcIndexFloor;
        output[i] = inputBuffer[srcIndexFloor] * (1 - fraction) + inputBuffer[srcIndexCeil] * fraction;
    }

    return output;
}

export function useRemoteParticipantTranslation({
    roomId,
    enabled,
    sttEnabled = true,  // STTëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•­ìƒ í™œì„±í™”
    sourceLanguage = 'ko',  // ë°œí™”ìê°€ ë§í•˜ëŠ” ì–¸ì–´ (ê¸°ë³¸: í•œêµ­ì–´)
    targetLanguage = 'en',  // ë“£ê³  ì‹¶ì€ ì–¸ì–´ (ê¸°ë³¸: ì˜ì–´)
    listenerId,
    autoPlayTTS = true,
    onTranscript,
    onError,
}: UseRemoteParticipantTranslationOptions): UseRemoteParticipantTranslationReturn {
    const [isActive, setIsActive] = useState(false);
    const [activeParticipantCount, setActiveParticipantCount] = useState(0);
    const [transcripts, setTranscripts] = useState<Map<string, RemoteTranscriptData>>(new Map());
    const [error, setError] = useState<Error | null>(null);

    const participants = useParticipants();
    const { localParticipant } = useLocalParticipant();

    // Track microphone tracks to know when they become available
    const audioTracks = useTracks([Track.Source.Microphone], { onlySubscribed: true });

    // All refs for stable references
    const streamsRef = useRef<Map<string, ParticipantStream>>(new Map());
    const creatingStreamsRef = useRef<Set<string>>(new Set());  // ì¤‘ë³µ ìƒì„± ë°©ì§€ìš© ë½
    const enabledRef = useRef(enabled);
    const sttEnabledRef = useRef(sttEnabled);
    const roomIdRef = useRef(roomId);
    const listenerIdRef = useRef(listenerId);
    const sourceLanguageRef = useRef(sourceLanguage);
    const targetLanguageRef = useRef(targetLanguage);
    const autoPlayTTSRef = useRef(autoPlayTTS);
    const onTranscriptRef = useRef(onTranscript);
    const onErrorRef = useRef(onError);
    const localParticipantIdRef = useRef<string | null>(null);
    const isInitializedRef = useRef(false);

    // Audio ducking
    const { duckParticipant, unduckParticipant, unduckAll } = useAudioDucking();
    const duckParticipantRef = useRef(duckParticipant);
    const unduckParticipantRef = useRef(unduckParticipant);
    const unduckAllRef = useRef(unduckAll);

    // TTS playback with ducking callbacks
    const { queueAudio, stopAllAudio } = useAudioPlayback({
        onPlayStart: (participantId) => {
            if (participantId) {
                console.log(`[RemoteTranslation] TTS started for ${participantId}, ducking...`);
                duckParticipantRef.current(participantId);
            }
        },
        onPlayEnd: (participantId) => {
            if (participantId) {
                console.log(`[RemoteTranslation] TTS ended for ${participantId}, unducking...`);
                unduckParticipantRef.current(participantId);
            }
        },
        onError: (err) => {
            console.error("[RemoteTranslation] Playback error:", err);
        },
    });
    const queueAudioRef = useRef(queueAudio);
    const stopAllAudioRef = useRef(stopAllAudio);

    // Keep all refs updated
    useEffect(() => {
        enabledRef.current = enabled;
        sttEnabledRef.current = sttEnabled;
        roomIdRef.current = roomId;
        listenerIdRef.current = listenerId;
        sourceLanguageRef.current = sourceLanguage;
        targetLanguageRef.current = targetLanguage;
        autoPlayTTSRef.current = autoPlayTTS;
        onTranscriptRef.current = onTranscript;
        onErrorRef.current = onError;
        duckParticipantRef.current = duckParticipant;
        unduckParticipantRef.current = unduckParticipant;
        unduckAllRef.current = unduckAll;
        queueAudioRef.current = queueAudio;
        stopAllAudioRef.current = stopAllAudio;
    }, [enabled, sttEnabled, roomId, listenerId, sourceLanguage, targetLanguage, autoPlayTTS, onTranscript, onError, duckParticipant, unduckParticipant, unduckAll, queueAudio, stopAllAudio]);

    // Update local participant identity ref
    useEffect(() => {
        if (localParticipant?.identity) {
            console.log(`[RemoteTranslation] Local participant identified: ${localParticipant.identity}`);
            localParticipantIdRef.current = localParticipant.identity;
        } else {
            console.log(`[RemoteTranslation] Local participant not ready yet:`, {
                localParticipant: !!localParticipant,
                identity: localParticipant?.identity,
            });
        }
    }, [localParticipant, localParticipant?.identity]);

    // Memoize participant IDs for stable comparison
    const participantIds = useMemo(
        () => participants.map(p => p.identity).sort().join(','),
        [participants]
    );

    // Memoize audio track info to detect when new tracks become available
    const audioTrackInfo = useMemo(
        () => audioTracks.map(t => `${t.participant.identity}:${t.publication?.trackSid || 'none'}`).sort().join(','),
        [audioTracks]
    );

    // Memoize remote participant metadata to detect language changes
    // This triggers stream re-creation when a participant updates their sourceLanguage
    const participantMetadataInfo = useMemo(
        () => participants
            .filter(p => p.identity !== localParticipant?.identity)
            .map(p => {
                const sourceLang = getParticipantSourceLanguage(p);
                return `${p.identity}:${sourceLang || 'none'}`;
            })
            .sort()
            .join(','),
        [participants, localParticipant?.identity]
    );

    // Track previous language settings to detect changes
    const prevSourceLanguageRef = useRef(sourceLanguage);
    const prevTargetLanguageRef = useRef(targetLanguage);
    const prevParticipantMetadataRef = useRef(participantMetadataInfo);

    // Helper functions using refs (not useCallback to avoid dependency issues)
    const cleanupParticipantStream = (participantId: string) => {
        const stream = streamsRef.current.get(participantId);
        if (!stream) return;

        console.log(`[RemoteTranslation] Cleaning up stream for ${participantId}`);

        // 0. ì¬ì—°ê²° íƒ€ì´ë¨¸ ì·¨ì†Œ
        if (stream.reconnectTimeoutId) {
            clearTimeout(stream.reconnectTimeoutId);
            stream.reconnectTimeoutId = null;
        }

        // 1. ë¶„ì„ ì¸í„°ë²Œ ì¤‘ì§€
        if (stream.analysisInterval) {
            clearInterval(stream.analysisInterval);
            stream.analysisInterval = null;
        }

        // 2. AudioWorklet í¬íŠ¸ ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ ì œê±° ë° ì—°ê²° í•´ì œ
        if (stream.workletNode) {
            stream.workletNode.port.onmessage = null;
            stream.workletNode.port.close();
            stream.workletNode.disconnect();
            stream.workletNode = null;
        }

        // 3. ì†ŒìŠ¤ ë…¸ë“œ ì—°ê²° í•´ì œ
        if (stream.sourceNode) {
            stream.sourceNode.disconnect();
            stream.sourceNode = null;
        }

        // 4. AudioContext ë‹«ê¸° (ëª¨ë“  ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ì§€)
        if (stream.audioContext && stream.audioContext.state !== 'closed') {
            stream.audioContext.close().catch(() => {
                // AudioContext close ì‹¤íŒ¨ ë¬´ì‹œ
            });
        }

        // 5. WebSocket ë‹«ê¸° (ì˜ë„ì  ì¢…ë£Œë¡œ í‘œì‹œí•˜ì—¬ ì¬ì—°ê²° ë°©ì§€)
        stream.isIntentionalClose = true;
        if (stream.ws) {
            if (stream.ws.readyState === WebSocket.OPEN || stream.ws.readyState === WebSocket.CONNECTING) {
                stream.ws.close();
            }
            stream.ws = null as any;
        }

        // 6. ë²„í¼ ë¹„ìš°ê¸°
        stream.audioBuffer = [];

        // 7. MediaStream ì •ë¦¬ (ì„ íƒì  - íŠ¸ë™ì€ LiveKitì´ ê´€ë¦¬)
        stream.mediaStream = null;

        streamsRef.current.delete(participantId);
        console.log(`[RemoteTranslation] Stream cleanup complete for ${participantId}`);
    };

    const cleanupAllStreams = () => {
        console.log(`[RemoteTranslation] Cleaning up all streams`);

        streamsRef.current.forEach((_, participantId) => {
            cleanupParticipantStream(participantId);
        });

        creatingStreamsRef.current.clear();  // ëª¨ë“  ë½ í•´ì œ
        unduckAllRef.current();
        stopAllAudioRef.current();
        setTranscripts(new Map());
    };

    // ë²„í¼ëœ ì˜¤ë””ì˜¤ ì „ì†¡ í—¬í¼ í•¨ìˆ˜
    const sendBufferedAudio = (stream: ParticipantStream, reason: string) => {
        if (stream.audioBuffer.length === 0) return;
        if (!stream.ws || stream.ws.readyState !== WebSocket.OPEN) return;
        if (!stream.isHandshakeComplete) return;

        const totalLength = stream.audioBuffer.reduce((sum, arr) => sum + arr.length, 0);
        if (totalLength === 0) return;

        // ë²„í¼ í•©ì¹˜ê¸°
        const combined = new Float32Array(totalLength);
        let offset = 0;
        for (const chunk of stream.audioBuffer) {
            combined.set(chunk, offset);
            offset += chunk.length;
        }
        stream.audioBuffer = [];

        // ë¦¬ìƒ˜í”Œë§ ë° ì „ì†¡
        const resampled = resample(combined, stream.audioContext.sampleRate, SAMPLE_RATE);

        // ì „ì†¡ ì „ ì˜¤ë””ì˜¤ ë ˆë²¨ í™•ì¸
        const sendRms = calculateRMS(resampled);
        const sendMax = Math.max(...Array.from(resampled).map(Math.abs));

        const int16Data = float32ToInt16(resampled);

        stream.ws.send(int16Data.buffer);
        console.log(`[RemoteTranslation] ${stream.participantId}: Sent ${int16Data.length} samples (${(int16Data.length / SAMPLE_RATE).toFixed(1)}s) - ${reason} | RMS=${sendRms.toFixed(6)}, Max=${sendMax.toFixed(6)}`);
    };

    // WebSocket ì¬ì—°ê²° í•¨ìˆ˜
    const reconnectWebSocket = (stream: ParticipantStream, participant: RemoteParticipant | LocalParticipant) => {
        if (stream.isIntentionalClose) {
            console.log(`[RemoteTranslation] ${stream.participantId}: Skipping reconnect (intentional close)`);
            return;
        }

        if (stream.reconnectAttempts >= WS_RECONNECT_MAX_RETRIES) {
            console.error(`[RemoteTranslation] ${stream.participantId}: Max reconnect attempts (${WS_RECONNECT_MAX_RETRIES}) reached, giving up`);
            cleanupParticipantStream(stream.participantId);
            return;
        }

        // ì§€ìˆ˜ ë°±ì˜¤í”„ ê³„ì‚°
        const delay = Math.min(
            WS_RECONNECT_BASE_DELAY_MS * Math.pow(2, stream.reconnectAttempts),
            WS_RECONNECT_MAX_DELAY_MS
        );

        stream.reconnectAttempts++;
        console.log(`[RemoteTranslation] ${stream.participantId}: ğŸ”„ Reconnecting in ${delay}ms (attempt ${stream.reconnectAttempts}/${WS_RECONNECT_MAX_RETRIES})`);

        stream.reconnectTimeoutId = setTimeout(() => {
            // ìŠ¤íŠ¸ë¦¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            const currentStream = streamsRef.current.get(stream.participantId);
            if (!currentStream || currentStream.isIntentionalClose) {
                console.log(`[RemoteTranslation] ${stream.participantId}: Stream no longer exists, cancelling reconnect`);
                return;
            }

            // MediaStreamì´ ì—¬ì „íˆ ìœ íš¨í•œì§€ í™•ì¸
            if (!currentStream.mediaStream || currentStream.mediaStream.getTracks().every(t => t.readyState === 'ended')) {
                console.log(`[RemoteTranslation] ${stream.participantId}: MediaStream no longer valid, cancelling reconnect`);
                cleanupParticipantStream(stream.participantId);
                return;
            }

            // ìƒˆ WebSocket ìƒì„±
            const actualListenerId = listenerIdRef.current || localParticipantIdRef.current || 'unknown';
            const wsUrl = `${WS_BASE_URL}?roomId=${encodeURIComponent(roomIdRef.current)}&listenerId=${encodeURIComponent(actualListenerId)}&sourceLang=${currentStream.remoteSourceLang}&targetLang=${targetLanguageRef.current}&participantId=${encodeURIComponent(stream.participantId)}`;

            console.log(`[RemoteTranslation] ${stream.participantId}: Creating new WebSocket connection`);
            const ws = new WebSocket(wsUrl);
            ws.binaryType = 'arraybuffer';

            currentStream.ws = ws;
            currentStream.isHandshakeComplete = false;

            setupWebSocketHandlers(ws, currentStream, participant);
        }, delay);
    };

    // WebSocket ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì • (ì¬ì—°ê²° ì‹œ ì¬ì‚¬ìš©)
    const setupWebSocketHandlers = (ws: WebSocket, stream: ParticipantStream, participant: RemoteParticipant | LocalParticipant) => {
        const participantId = stream.participantId;

        ws.onopen = async () => {
            console.log(`[RemoteTranslation] ${participantId}: WebSocket opened, sending handshake`);

            // ì¬ì—°ê²° ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
            stream.reconnectAttempts = 0;

            // Send metadata header
            const metadata = createMetadataHeader(SAMPLE_RATE, 1, 16);
            ws.send(metadata);
        };

        ws.onmessage = (event) => {
            const currentStream = streamsRef.current.get(participantId);
            if (!currentStream) return;

            // Handshake response
            if (!currentStream.isHandshakeComplete && typeof event.data === 'string') {
                try {
                    const response = JSON.parse(event.data);
                    if (response.status === 'ready') {
                        console.log(`[RemoteTranslation] ${participantId}: Handshake complete`);
                        currentStream.isHandshakeComplete = true;

                        // Start audio capture after handshake (ì¬ì—°ê²° ì‹œì—ë„)
                        if (!currentStream.sourceNode && currentStream.mediaStream) {
                            startAudioCapture(participantId, currentStream.mediaStream);
                        }
                    }
                } catch (e) {
                    console.error(`[RemoteTranslation] ${participantId}: Failed to parse handshake:`, e);
                }
                return;
            }

            // Transcript message
            if (typeof event.data === 'string') {
                try {
                    const data = JSON.parse(event.data);
                    if (data.type === 'transcript') {
                        // ì ‘ë‘ì‚¬ ì œê±° í•¨ìˆ˜
                        const stripPrefixes = (text: string | undefined): string => {
                            if (!text) return '';
                            return text
                                .replace(/^\[FINAL\]\s*/i, '')
                                .replace(/^\[LLM\]\s*/i, '')
                                .replace(/^\[PARTIAL\]\s*/i, '')
                                .trim();
                        };

                        const transcriptData: RemoteTranscriptData = {
                            participantId: data.participantId || participantId,
                            participantName: participant.name || participantId,
                            profileImg: getParticipantProfileImg(participant),
                            original: stripPrefixes(data.original) || stripPrefixes(data.text),
                            translated: stripPrefixes(data.translated),
                            isFinal: data.isFinal,
                        };

                        console.log(`[RemoteTranslation] ${participantId}: Transcript received:`, transcriptData);

                        setTranscripts(prev => {
                            const newMap = new Map(prev);
                            newMap.set(participantId, transcriptData);
                            return newMap;
                        });

                        onTranscriptRef.current?.(transcriptData);
                    }
                } catch (e) {
                    console.error(`[RemoteTranslation] ${participantId}: Failed to parse message:`, e);
                }
            } else if (event.data instanceof ArrayBuffer) {
                // TTS audio (MP3 format) - only play when translation mode is enabled AND not for local participant
                console.log(`[RemoteTranslation] ${participantId}: Received TTS audio (MP3):`, event.data.byteLength, "bytes");
                const currentIsLocal = participantId === localParticipantIdRef.current;
                if (autoPlayTTSRef.current && enabledRef.current && !currentIsLocal) {
                    // Don't pass sampleRate to use MP3 decoding instead of PCM
                    queueAudioRef.current(event.data, undefined, participantId);
                } else if (currentIsLocal) {
                    console.log(`[RemoteTranslation] ${participantId}: Skipping TTS for local participant`);
                }
            }
        };

        ws.onerror = (event) => {
            console.error(`[RemoteTranslation] ${participantId}: WebSocket error:`, event);
            const err = new Error(`WebSocket error for ${participantId}`);
            setError(err);
            onErrorRef.current?.(err);
        };

        ws.onclose = (event) => {
            console.log(`[RemoteTranslation] ${participantId}: WebSocket closed (code: ${event.code}, reason: ${event.reason})`);

            const currentStream = streamsRef.current.get(participantId);
            if (!currentStream) return;

            // ì˜ë„ì  ì¢…ë£Œê°€ ì•„ë‹ˆë©´ ì¬ì—°ê²° ì‹œë„
            if (!currentStream.isIntentionalClose) {
                console.log(`[RemoteTranslation] ${participantId}: Unexpected close, attempting reconnect...`);
                reconnectWebSocket(currentStream, participant);
            } else {
                console.log(`[RemoteTranslation] ${participantId}: Intentional close, no reconnect`);
                streamsRef.current.delete(participantId);
            }
        };
    };

    const startAudioCapture = async (participantId: string, mediaStream: MediaStream) => {
        const stream = streamsRef.current.get(participantId);
        if (!stream || !stream.audioContext) return;

        try {
            // Load AudioWorklet
            await stream.audioContext.audioWorklet.addModule('/audio-processor.js');

            // Create source node
            const sourceNode = stream.audioContext.createMediaStreamSource(mediaStream);
            stream.sourceNode = sourceNode;

            // Create worklet node
            const workletNode = new AudioWorkletNode(stream.audioContext, 'audio-processor');
            stream.workletNode = workletNode;

            // Handle audio data with debugging
            workletNode.port.onmessage = (event) => {
                // ìŠ¤íŠ¸ë¦¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                const currentStream = streamsRef.current.get(participantId);
                if (!currentStream || !currentStream.workletNode) {
                    return; // ìŠ¤íŠ¸ë¦¼ì´ ì •ë¦¬ë˜ì—ˆìœ¼ë©´ ë¬´ì‹œ
                }

                // ë””ë²„ê·¸ ë©”ì‹œì§€ ì²˜ë¦¬
                if (event.data.debug) {
                    // ì •ë¦¬ëœ í›„ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
                    if (!streamsRef.current.has(participantId)) return;
                    console.log(`[AudioWorklet] ${participantId}: ${event.data.message}`);
                    return;
                }

                const { audioData, rms } = event.data;
                if (audioData && currentStream.audioBuffer) {
                    // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹… (ê°€ë”)
                    if (Math.random() < 0.02) {  // 2% í™•ë¥ ë¡œ ë¡œê¹…
                        console.log(`[AudioCapture] ${participantId}: buffer RMS=${rms?.toFixed(6) || 'N/A'}, samples=${audioData.length}`);
                    }
                    currentStream.audioBuffer.push(new Float32Array(audioData));
                }
            };

            // Connect (NOT to destination)
            sourceNode.connect(workletNode);

            // ì£¼ê¸°ì  ë²„í¼ ë¶„ì„ ë° í•˜ì´ë¸Œë¦¬ë“œ ì „ì†¡ (ì¹¨ë¬µ ê°ì§€ + ê°•ì œ ì „ì†¡)
            stream.analysisInterval = setInterval(() => {
                if (stream.audioBuffer.length === 0) return;
                if (!stream.ws || stream.ws.readyState !== WebSocket.OPEN) return;
                if (!stream.isHandshakeComplete) return;

                const now = Date.now();

                // ë²„í¼ ì „ì²´ì˜ RMS ê³„ì‚° (ìµœê·¼ ì²­í¬ë“¤ë§Œ)
                const recentChunks = stream.audioBuffer.slice(-5);  // ìµœê·¼ 5ê°œ ì²­í¬ë§Œ ë¶„ì„
                const recentSamples: number[] = [];
                for (const chunk of recentChunks) {
                    for (let i = 0; i < chunk.length; i++) {
                        recentSamples.push(chunk[i]);
                    }
                }
                const rms = calculateRMS(new Float32Array(recentSamples));

                // ì´ ë²„í¼ í¬ê¸°
                const totalSamples = stream.audioBuffer.reduce((sum, arr) => sum + arr.length, 0);
                const resampledSamples = Math.floor(totalSamples * SAMPLE_RATE / stream.audioContext.sampleRate);
                const timeSinceLastSend = now - stream.lastSendTime;

                // ë°œí™” ê°ì§€ ìƒíƒœ ì—…ë°ì´íŠ¸
                if (rms >= SPEECH_THRESHOLD) {
                    if (!stream.isSpeaking) {
                        console.log(`[RemoteTranslation] ${participantId}: Speech detected (RMS: ${rms.toFixed(4)})`);
                    }
                    stream.isSpeaking = true;
                    stream.silenceStartTime = null;
                    stream.lastSpeechTime = now;
                } else if (rms < SILENCE_THRESHOLD) {
                    if (stream.isSpeaking && stream.silenceStartTime === null) {
                        stream.silenceStartTime = now;
                    }
                }

                // ============================================
                // ì „ì†¡ ì¡°ê±´ (ìš°ì„ ìˆœìœ„ ìˆœ, ì˜¤ë””ì˜¤ ì†ì‹¤ ë°©ì§€)
                // ============================================

                let shouldSend = false;
                let sendReason = '';

                // 1. ë°œí™” í›„ ì¹¨ë¬µ ê°ì§€ â†’ ì¦‰ì‹œ ì „ì†¡ (ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ)
                if (stream.isSpeaking && stream.silenceStartTime !== null) {
                    const silenceDuration = now - stream.silenceStartTime;
                    if (silenceDuration >= SILENCE_DURATION_MS) {
                        shouldSend = true;
                        sendReason = `utterance complete (silence: ${silenceDuration}ms)`;
                        stream.isSpeaking = false;
                        stream.silenceStartTime = null;
                    }
                }

                // 2. ê°•ì œ ì „ì†¡ ê°„ê²© ì´ˆê³¼ â†’ ë¬´ì¡°ê±´ ì „ì†¡ (ì˜¤ë””ì˜¤ ì†ì‹¤ ë°©ì§€)
                if (!shouldSend && timeSinceLastSend >= FORCED_SEND_INTERVAL_MS) {
                    shouldSend = true;
                    sendReason = `forced send (${(timeSinceLastSend / 1000).toFixed(1)}s elapsed)`;
                }

                // 3. ìµœëŒ€ ë²„í¼ í¬ê¸° ì´ˆê³¼ â†’ ê°•ì œ ì „ì†¡ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                if (!shouldSend && resampledSamples >= MAX_BUFFER_SAMPLES) {
                    shouldSend = true;
                    sendReason = `buffer full (${resampledSamples} samples)`;
                    stream.isSpeaking = false;
                    stream.silenceStartTime = null;
                }

                // ì „ì†¡ ì‹¤í–‰
                if (shouldSend) {
                    sendBufferedAudio(stream, sendReason);
                    stream.lastSendTime = now;
                }
            }, ANALYSIS_INTERVAL_MS);

            console.log(`[RemoteTranslation] ${participantId}: Audio capture started (hybrid mode: silence detection + forced ${FORCED_SEND_INTERVAL_MS}ms)`);

        } catch (err) {
            console.error(`[RemoteTranslation] ${participantId}: Failed to start audio capture:`, err);
        }
    };

    const createParticipantStream = async (participant: RemoteParticipant | LocalParticipant) => {
        if (!participant) return;

        const participantId = participant.identity;

        // ë¡œì»¬ ì°¸ê°€ì ì²´í¬ (ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ í™•ì¸)
        const isLocalByIdentity = participantId === localParticipantIdRef.current;
        const isLocalByProperty = 'isLocal' in participant && participant.isLocal === true;

        console.log(`[RemoteTranslation] Checking participant: ${participantId}`, {
            isLocalByIdentity,
            isLocalByProperty,
            myIdentity: localParticipantIdRef.current,
        });

        // Skip local participant - we only translate remote participants
        if (isLocalByIdentity || isLocalByProperty) {
            console.log(`[RemoteTranslation] âŒ Skipping LOCAL participant: ${participantId}`);
            return;
        }

        console.log(`[RemoteTranslation] âœ“ Processing REMOTE participant: ${participantId}`);

        // Check if already exists or being created (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        if (streamsRef.current.has(participantId)) {
            console.log(`[RemoteTranslation] Stream already exists for ${participantId}`);
            return;
        }

        if (creatingStreamsRef.current.has(participantId)) {
            console.log(`[RemoteTranslation] Stream already being created for ${participantId}`);
            return;
        }

        // ë½ íšë“
        creatingStreamsRef.current.add(participantId);
        console.log(`[RemoteTranslation] Lock acquired for ${participantId}`);

        // Get microphone track
        const micPub = participant.getTrackPublication(Track.Source.Microphone);
        if (!micPub?.track?.mediaStreamTrack) {
            console.log(`[RemoteTranslation] ${participantId}: No microphone track available`);
            creatingStreamsRef.current.delete(participantId);  // ë½ í•´ì œ
            return;
        }

        // Debug: íŠ¸ë™ ì •ë³´ í™•ì¸
        const mediaStreamTrack = micPub.track.mediaStreamTrack;
        console.log(`[RemoteTranslation] ${participantId}: Track info:`, {
            participantIdentity: participant.identity,
            participantIsLocal: 'isLocal' in participant ? (participant as any).isLocal : 'N/A',
            trackSid: micPub.trackSid,
            trackSource: micPub.source,
            isSubscribed: micPub.isSubscribed,
            isEnabled: micPub.isEnabled,
            // MediaStreamTrack ìƒì„¸ ì •ë³´
            mediaTrackId: mediaStreamTrack.id,
            mediaTrackKind: mediaStreamTrack.kind,
            mediaTrackLabel: mediaStreamTrack.label,
            mediaTrackEnabled: mediaStreamTrack.enabled,
            mediaTrackMuted: mediaStreamTrack.muted,
            mediaTrackReadyState: mediaStreamTrack.readyState,
        });

        try {
            // ì›ê²© ì°¸ê°€ìì˜ sourceLanguageë¥¼ ê·¸ë“¤ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜´
            const remoteSourceLangFromMetadata = getParticipantSourceLanguage(participant);
            const myTargetLang = targetLanguageRef.current;

            // ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
            if (remoteSourceLangFromMetadata === null) {
                console.log(`[RemoteTranslation] âš ï¸ ${participantId}: No sourceLanguage in metadata, using default 'ko'`);
                console.log(`[RemoteTranslation] ${participantId}: metadata = ${participant.metadata}`);
            }

            // ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 'ko' ì‚¬ìš© (í•œêµ­ì–´ê°€ ê°€ì¥ í”í•¨)
            // ë©”íƒ€ë°ì´í„°ê°€ ìˆì§€ë§Œ ë‚´ íƒ€ê²Ÿ ì–¸ì–´ì™€ ê°™ìœ¼ë©´ ë²ˆì—­ ë¶ˆí•„ìš”
            const remoteSourceLang = remoteSourceLangFromMetadata || 'ko';

            // ê°™ì€ ì–¸ì–´ë©´ ë²ˆì—­ ë¶ˆí•„ìš” - ìŠ¤íŠ¸ë¦¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
            // ë‹¨, ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ì¼ë‹¨ ìŠ¤íŠ¸ë¦¼ ìƒì„± (ë‚˜ì¤‘ì— ë©”íƒ€ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŒ)
            if (remoteSourceLangFromMetadata !== null && remoteSourceLang === myTargetLang) {
                console.log(`[RemoteTranslation] â­ï¸ Skipping ${participantId}: same language (${remoteSourceLang} == ${myTargetLang})`);
                creatingStreamsRef.current.delete(participantId);  // ë½ í•´ì œ
                return;
            }

            console.log(`[RemoteTranslation] Creating stream for ${participantId}`, {
                remoteSourceLang,  // ì›ê²© ì°¸ê°€ìê°€ ë§í•˜ëŠ” ì–¸ì–´
                remoteSourceLangFromMetadata,  // ë©”íƒ€ë°ì´í„°ì—ì„œ ì½ì€ ê°’ (null ê°€ëŠ¥)
                myTargetLang,  // ë‚´ê°€ ë“£ê³  ì‹¶ì€ ì–¸ì–´
                participantMetadata: participant.metadata,
            });

            // Create WebSocket with roomId, listenerId, participantId and language params
            // roomId = ë°© ID (ê°™ì€ ë°©ì˜ ë™ì¼ ì–¸ì–´ ê·¸ë£¹ì„ ë¬¶ê¸° ìœ„í•´)
            // listenerId = ë“£ëŠ” ì‚¬ëŒì˜ ID (ë¡œì»¬ ì‚¬ìš©ì)
            // sourceLang = ì›ê²© ì°¸ê°€ìê°€ ë§í•˜ëŠ” ì–¸ì–´ (ê·¸ë“¤ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ)
            // targetLang = ë‚´ê°€ ë“£ê³  ì‹¶ì€ ì–¸ì–´ (ë²ˆì—­ ëŒ€ìƒ)
            const actualListenerId = listenerIdRef.current || localParticipantIdRef.current || 'unknown';
            const wsUrl = `${WS_BASE_URL}?roomId=${encodeURIComponent(roomIdRef.current)}&listenerId=${encodeURIComponent(actualListenerId)}&sourceLang=${remoteSourceLang}&targetLang=${targetLanguageRef.current}&participantId=${encodeURIComponent(participantId)}`;
            const ws = new WebSocket(wsUrl);
            ws.binaryType = 'arraybuffer';

            // Create AudioContext
            const audioContext = new AudioContext();
            const mediaStream = new MediaStream([micPub.track.mediaStreamTrack]);

            const stream: ParticipantStream = {
                participantId,
                ws,
                audioContext,
                sourceNode: null,
                workletNode: null,
                audioBuffer: [],
                analysisInterval: null,
                isHandshakeComplete: false,
                // ì¹¨ë¬µ ê°ì§€ ìƒíƒœ ì´ˆê¸°í™”
                isSpeaking: false,
                silenceStartTime: null,
                lastSpeechTime: Date.now(),
                // ê°•ì œ ì „ì†¡ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
                lastSendTime: Date.now(),
                // WebSocket ì¬ì—°ê²° ìƒíƒœ ì´ˆê¸°í™”
                reconnectAttempts: 0,
                reconnectTimeoutId: null,
                isIntentionalClose: false,
                mediaStream,
                remoteSourceLang,
            };

            streamsRef.current.set(participantId, stream);
            creatingStreamsRef.current.delete(participantId);  // ë½ í•´ì œ (ìŠ¤íŠ¸ë¦¼ ë“±ë¡ ì™„ë£Œ)
            console.log(`[RemoteTranslation] Lock released for ${participantId} (stream registered)`);

            // WebSocket í•¸ë“¤ëŸ¬ ì„¤ì • (ì¬ì—°ê²° ì‹œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ ì‚¬ìš©)
            setupWebSocketHandlers(ws, stream, participant);

        } catch (err) {
            console.error(`[RemoteTranslation] ${participantId}: Failed to create stream:`, err);
            const error = err instanceof Error ? err : new Error(`Failed to create stream for ${participantId}`);
            setError(error);
            onErrorRef.current?.(error);
            creatingStreamsRef.current.delete(participantId);  // ë½ í•´ì œ
        }
    };

    // Main effect: Manage streams based on sttEnabled, participant changes, and language changes
    useEffect(() => {
        // Get local participant ID directly from the hook (more reliable than ref)
        const localId = localParticipant?.identity;

        // Wait for local participant to be identified
        if (!localId) {
            console.log(`[RemoteTranslation] Waiting for local participant to be identified...`);
            return;
        }

        // Update ref for use in other functions
        localParticipantIdRef.current = localId;

        if (!sttEnabled) {
            console.log(`[RemoteTranslation] Stopping STT`);
            cleanupAllStreams();
            setIsActive(false);
            prevSourceLanguageRef.current = sourceLanguage;
            prevTargetLanguageRef.current = targetLanguage;
            return;
        }

        // Check if language has changed - if so, recreate all streams
        const languageChanged =
            prevSourceLanguageRef.current !== sourceLanguage ||
            prevTargetLanguageRef.current !== targetLanguage;

        if (languageChanged && streamsRef.current.size > 0) {
            console.log(`[RemoteTranslation] Language changed: ${prevSourceLanguageRef.current}->${sourceLanguage}, ${prevTargetLanguageRef.current}->${targetLanguage}`);
            console.log(`[RemoteTranslation] Recreating all streams with new language settings...`);
            cleanupAllStreams();
        }

        // Check if any participant's metadata (sourceLanguage) has changed
        // This handles the case where a participant updates their language after joining
        const metadataChanged = prevParticipantMetadataRef.current !== participantMetadataInfo;
        if (metadataChanged && streamsRef.current.size > 0) {
            console.log(`[RemoteTranslation] Participant metadata changed:`);
            console.log(`  Before: ${prevParticipantMetadataRef.current}`);
            console.log(`  After: ${participantMetadataInfo}`);

            // Find which participants changed their metadata
            const prevMap = new Map(
                prevParticipantMetadataRef.current.split(',').filter(Boolean).map(s => {
                    const [id, lang] = s.split(':');
                    return [id, lang] as [string, string];
                })
            );
            const currentMap = new Map(
                participantMetadataInfo.split(',').filter(Boolean).map(s => {
                    const [id, lang] = s.split(':');
                    return [id, lang] as [string, string];
                })
            );

            // Cleanup and recreate streams for participants whose language changed
            currentMap.forEach((newLang, participantId) => {
                const prevLang = prevMap.get(participantId);
                if (prevLang !== newLang) {
                    console.log(`[RemoteTranslation] ${participantId}: language changed ${prevLang} -> ${newLang}, recreating stream`);
                    cleanupParticipantStream(participantId);
                    // Stream will be recreated below in the "Add new participants" section
                }
            });
        }

        prevSourceLanguageRef.current = sourceLanguage;
        prevTargetLanguageRef.current = targetLanguage;
        prevParticipantMetadataRef.current = participantMetadataInfo;

        // Parse participant IDs from the memoized string
        const currentIds = participantIds ? participantIds.split(',').filter(Boolean) : [];

        // Count remote participants only (exclude local)
        const remoteIds = currentIds.filter(id => id !== localId);
        console.log(`[RemoteTranslation] Local: ${localId}, Remote participants: [${remoteIds.join(', ')}], Audio tracks: ${audioTracks.length}`);
        setIsActive(true);

        const currentParticipantIds = new Set(currentIds);

        // Find REMOTE participants with available audio tracks
        const remoteAudioTracks = audioTracks.filter(t =>
            t.participant.identity !== localId &&
            t.publication?.track?.mediaStreamTrack
        );

        console.log(`[RemoteTranslation] Available remote audio tracks:`, remoteAudioTracks.map(t => ({
            identity: t.participant.identity,
            trackSid: t.publication?.trackSid,
            hasMediaStreamTrack: !!t.publication?.track?.mediaStreamTrack,
        })));

        // Add new remote participants that have audio tracks available
        // NOTE: Check against streamsRef.current directly (not a snapshot) to handle
        // cases where streams were just cleaned up due to metadata changes
        remoteAudioTracks.forEach(trackRef => {
            const participantId = trackRef.participant.identity;
            // Check current state of streams, not a pre-cleanup snapshot
            if (!streamsRef.current.has(participantId)) {
                console.log(`[RemoteTranslation] Creating stream for REMOTE: ${participantId} (I am: ${localId})`);
                createParticipantStream(trackRef.participant as RemoteParticipant);
            }
        });

        // Remove departed participants
        // Check current streams and remove those whose participants are no longer in the room
        streamsRef.current.forEach((_, participantId) => {
            if (!currentParticipantIds.has(participantId)) {
                console.log(`[RemoteTranslation] Participant left: ${participantId}`);
                cleanupParticipantStream(participantId);
            }
        });

        // Count how many remote participants have active streams
        const activeRemoteStreams = Array.from(streamsRef.current.keys()).filter(id => id !== localId);
        setActiveParticipantCount(activeRemoteStreams.length);

        // Cleanup on unmount or when sttEnabled changes
        return () => {
            // Only cleanup if sttEnabled is being turned off
            if (!sttEnabledRef.current) {
                cleanupAllStreams();
            }
        };
    // Depend on sttEnabled, participantIds, language changes, localParticipant identity, audio tracks, and participant metadata
    // eslint-disable-next-line react-hooks/exhaustive-deps
    }, [sttEnabled, participantIds, sourceLanguage, targetLanguage, localParticipant?.identity, audioTrackInfo, participantMetadataInfo]);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            cleanupAllStreams();
        };
    // eslint-disable-next-line react-hooks/exhaustive-deps
    }, []);

    return {
        isActive,
        activeParticipantCount,
        transcripts,
        error,
    };
}
